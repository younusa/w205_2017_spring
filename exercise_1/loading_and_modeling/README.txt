Loading and Modeling

In order to succesfully run load_data_lakes.sh for exercise 1, first run a UCB AMI instance. Second, run the start_hadoop.sh script to start hadoop. At this point, switch to the w205 user, clone git repository and run the load_data_lakes.sh script that is embedded im the loading_and_modeling folder. This script will download and rename the relevant csv data needed to complete exercise 1.

to transform csv data to tables that are stored in hdfs, run the hive_base_ddl.sql
